# MetaPro:  Meta-analysis pipeline for Transcriptomics + Genomics
---
The MetaPro Meta Transcriptomics/Genomics Pipeline is a software tool that will perform Transcriptomics and Genomic analysis
on Paired and Single Reads of FASTQ data.

# How to install
---
This package is meant to work in conjunction with Docker/Singularity.  All of the prerequisite tools, including the pipeline code
is delivered via the Docker Hub. https://hub.docker.com/r/billyc59/parkinson_pipeline/
Alternatively, individual parts of the pipeline are avaiable from this Github repository.

Therefore, to use this pipeline, Docker (https://www.docker.com/) or Singularity (https://www.sylabs.io/guides/2.6/user-guide/) is needed.

# How to use
---
This pipeline is a python script.  The script expects parameters in its invocation to launch properly:
- -c or --config: (optional) Path to config file (1)
- -1 or --pair1: Path to the file containing forward paired-end reads (in FASTQ only) (2)
- -2 or --pair2: Path to the file containing reverse paired-end reads (in FASTQ only) (2)
- -s or --single: Path to the file containing single-end reads (in FASTQ only) (2)
- -o or --output_folder: Path to the folder for the output of the pipeline
- -t or --num_threads: (optional) Maximum number of threads used by the pipeline
- --nhost: (optional) Skip the host-removal stage of the pipeline (3)
- --verbose_mode: (optional) Decide what to do with the interim files generated by the pipeline:  keep, compress, quiet (delete them) (4)


Notes:
1) If the config file is present, the settings inside the config file take precedence, and override the command line arguments when applicable.  It is possible to mix the config.ini and command line arguments together.  The config file will only use what is present inside it.  If a configuration is not present, the missing section will refer to its default location within the docker container. 
2) If the input data is single-ended, then only use the --single option, and do not include -1 and -2.  If the data is paired-ended, only use -1 and -2.  Do not mix the 2 options.  They are mutually exclusive
3) The default setting for the nhost option (if the flag is not present) is to include the host removal stage
4) verbose_mode in compression adds additional overhead to the runtime of the pipeline.  All files and subdirectories of every stage are being zipped to a location.  To speed up the pipeline, set this option to "keep".  Keep will leave the interim files in place.  Verbose-mode: quiet will also incur additional overhead, as it is deleting the data. 

The pipeline includes an optional config.ini file.  The user is meant to change the file to point to the location of local files and Databases as the pipeline is not distributed with
databases.
The config file is written with Python's ConfigParser, using the basic interpretation syntax.  
The following is an outline of each section of the config file:

## Databases
---
We intentionally did not include any database files in this distribution so as to allow the user more flexibility in how they want to use the pipeline.  Below is a description of each of the fields we used.  
* database_path
This field isn't apart of the parameters that the pipeline accepts.  It's a shortcut argument that makes filling the path to each database easier.

* Univec_Core
The Univec_Core Database is used in the Vector Contaminents removal stage.  A copy can be found at: https://www.ncbi.nlm.nih.gov/tools/vecscreen/univec/
* Adapter
The Adapter Database is used by Trimmomatic to trim Adapter segments out of the sequence files
A copy can be found inside the Trimmomatic tool installer, located at: http://www.usadellab.org/cms/?page=trimmomatic
This pipeline was built and tested using the TruSeq3-PE-2.fa Adapter Database
* Host
The Host Database is used to filter out Host Contaminents from your seuqence file.  You will need to change this with the CDS database of whichever animal was used in your experiment.
We get our CDS databases from the NCBI, eg: ftp://ftp.ncbi.nlm.nih.gov/pub/CCDS/current_human
* Rfam
The Rfam Database is used by Infernal, the rRNA filter.  
A copy can be found here: http://rfam.xfam.org/
* DNA_DB
The DNA DB is what we use to annotate the sequence data against.  We use the ChocoPhlAn database.
A copy can be found at: http://huttenhower.sph.harvard.edu/humann2_data/chocophlan/chocophlan.tar.gz
* DNA_DB_Split
The ChocoPhlAn database is big.  We split it up and process the chunks simultaneously.  The pipeline will split it and dump the chunks at this location
* Prot_DB
The Prot_DB is the protein db.  We use the non-redundant database from NCBI.  It will need to be indexed by DIAMOND before usage. (see DIAMOND for more details: https://github.com/bbuchfink/diamond)
It can be found here: ftp://ftp.ncbi.nlm.nih.gov/blast/db/
* accession2taxid
This database links each accession to a taxid number.  It's used as part of a custom program in the pipeline.
It can be found at: ftp://ftp.ncbi.nih.gov/pub/taxonomy/accession2taxid/
* nodes
This file is used in various parts in the pipeline.
It can be found at: ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdmp.zip
* names
It can be found at the same location as nodes (above)
* Kaiju_db
The Kaiju Database is used by Kaiju for taxonomic annotation.  Kaiju requires that the database be indexed before usage.  
Please see: https://github.com/bioinformatics-centre/kaiju/blob/master/README.md for more information.  Once the indexing is complete, the path to the database needs to be provided in this location
* Centrifuge_db
The Centrifuge Database is used for the Centrifuge tool, which is a part of the enzyme annotation phase.  More information can be found here: https://ccb.jhu.edu/software/centrifuge/manual.shtml
We use the Nucleotide Database, after it has been indexed.  Details can be found at the link to Centrifuge
* SWISS_PROT
SWISS_PROT is the SWISS Prot database, now called UniProt.  We use it in DIAMOND during the enzyme annotation stage.  A copy can be found here: https://www.uniprot.org/downloads
Note that this also needs to be indexed by DIAMOND prior to use.
* SWISS_PROT_map
This is a special file generated by a setup script we have included with the pipeline.  To obtain this file, copy the file "Download_Annotated_EC_mapping.py" from /pipeline/setup_scripts to a 
desired location, and run it.  The script will generate the tab-separated-value map file needed for the pipeline
* PriamDB
The database used by PRIAM.  To obtain this, the user needs to download a distribution of PRIAM.  The PriamDB path is looking for the location where the following files are found:
- PROFILES folder
- annotation_rules.xml
- genome_rules.xml
PRIAM can be downloaded here: http://priam.prabi.fr/
* DetectDB
Detect is an enzyme annotation tool. 


# Important Features
---
This pipeline was built with a few key features that make it easier to use than other pipes.
## Verbose-mode
This pipeline will produce many interim files along the way.  In an effort to maintain transparency, and allow for debugging and inspection of stages, the interim files have been saved, and compressed at each stage of the pipeline.  If the user wishes to avoid compression (saving roughly 1/100th of the runtime), the compress flag argument should be set to "verbose".  Otherwise, the default is to compress the interim files.

## Auto-resume
The pipeline is capable of skipping any and all stages it has run before.  If the user wishes to run a subset of stages, the user has to remove the stage folders, and any compressed files of the accompanying stage, and the pipeline will re-run the missing stages.  Note:  The removed stages do not have to be contiguous, but it is recommended that they are, to ensure the accuracy of the pipeline results.  Eg: a pipeline runs A -> B -> C -> D -> E.  If A, and D are removed, The pipeline will re-run A and D, leaving B, C, and E alone.  However, if D depends on C, then the changes from A -> B -> C will not propagate downward.  This feature also has the benefit of being able to resume running if the pipeline run was operating on a job-scheduler-controlled system, and inadequate time was allocated to the job.  

## Auto-death
In an effort to save computational resources, the pipeline will shut itself down if the dependencies of a stage are not met.  This is to ensure that if a pipeline run is part of a batch-processing scheduler, the pipeline will not continue to waste resources on an erroneous job.  

# Parallelism in the pipeline
---
The pipeline operates in a singularity machine.  As of writing (Sept 28, 2018), singularity does not support multi-machine parallelism.  This pipeline does not utilize MPI, but instead strives to use all the cores made available by the singularity machine through the Python Multiprocessing module.  To increase the performance of the pipeline, more cores should be given to the host machine, and increasing the number of cores the pipeline is allowed to use.
